```python
# 解压数据集
!unzip -oq data/data72793/lemon_homework.zip -d data/
!unzip -oq data/lemon_homework/lemon_lesson.zip -d data/
!unzip -oq data/lemon_lesson/train_images.zip -d data/
!unzip -oq data/lemon_lesson/test_images.zip -d data/
```


```python
# 导入所需要的库
from sklearn.utils import shuffle
import os
import pandas as pd
import numpy as np
from PIL import Image

import paddle
import paddle.nn as nn
from paddle.io import Dataset
import paddle.vision.transforms as T
import paddle.nn.functional as F
from paddle.metric import Accuracy

import warnings
warnings.filterwarnings("ignore")
```


```python
# 读取数据
train_images = pd.read_csv('data/lemon_lesson/train_images.csv', usecols=['id','class_num'])
#这里读出的是全部训练集的'id','class_num'信息

# labelshuffling 这里是打乱顺序，我们是对存放图片名称的文本进行内容的打乱。

def labelShuffling(dataFrame, groupByName = 'class_num'):

    groupDataFrame = dataFrame.groupby(by=[groupByName])
    labels = groupDataFrame.size()
    print("length of label is ", len(labels))
    maxNum = max(labels)
    lst = pd.DataFrame()
    for i in range(len(labels)):
        print("Processing label  :", i)
        tmpGroupBy = groupDataFrame.get_group(i)
        createdShuffleLabels = np.random.permutation(np.array(range(maxNum))) % labels[i]
        print("Num of the label is : ", labels[i])
        lst=lst.append(tmpGroupBy.iloc[createdShuffleLabels], ignore_index=True)
        print("Done")
    # lst.to_csv('test1.csv', index=False)
    return lst
```


```python
# 划分训练集和校验集
all_size = len(train_images)
# print(all_size)
train_size = int(all_size * 0.8)
train_image_list = train_images[:train_size]
val_image_list = train_images[train_size:]
```


```python
df = labelShuffling(train_image_list)
df = shuffle(df)

train_image_path_list = df['id'].values
label_list = df['class_num'].values
label_list = paddle.to_tensor(label_list, dtype='int64')
train_label_list = paddle.nn.functional.one_hot(label_list, num_classes=4)

val_image_path_list = val_image_list['id'].values
val_label_list = val_image_list['class_num'].values
val_label_list = paddle.to_tensor(val_label_list, dtype='int64')
val_label_list = paddle.nn.functional.one_hot(val_label_list, num_classes=4)

# 定义数据预处理
data_transforms = T.Compose([
    T.Resize(size=(224)),
    T.RandomHorizontalFlip(224),
    T.RandomVerticalFlip(224),
    T.Transpose(),    # HWC -> CHW
    T.Normalize(
        mean=[0, 0, 0],        # 归一化
        std=[255, 255, 255],
        to_rgb=True)    
    ])
```

    length of label is  4
    Processing label  : 0
    Num of the label is :  321
    Done
    Processing label  : 1
    Num of the label is :  207
    Done
    Processing label  : 2
    Num of the label is :  181
    Done
    Processing label  : 3
    Num of the label is :  172
    Done



```python
# 构建Dataset
class MyDataset(paddle.io.Dataset):
    """
    步骤一：继承paddle.io.Dataset类
    """
    def __init__(self, train_img_list, val_img_list,train_label_list,val_label_list, mode='train'):
        """
        步骤二：实现构造函数，定义数据读取方式，划分训练和测试数据集
        """
        super(MyDataset, self).__init__()
        self.img = []
        self.label = []
        # 借助pandas读csv的库，将前面得到的标签，名称list分别赋值
        self.train_images = train_img_list
        self.test_images = val_img_list
        self.train_label = train_label_list
        self.test_label = val_label_list
        if mode == 'train':
            # 读train_images的数据，这里是读训练的，所以括号里的是train的label和images
            for img,la in zip(self.train_images, self.train_label):
                self.img.append('data/train_images/'+img)
                self.label.append(la)
        else:
            # 读test_images的数据，这里是读验证的，所以括号里的应该是是val的label和images。
            for img,la in zip(self.test_images, self.test_label):
                #注意这里，因为我们从头到这里处理的都是list文件，本质上的test是从train划分出来的，所以路径应该为train_images
                self.img.append('data/train_images/'+img)
                self.label.append(la)

    def load_img(self, image_path):
        # 实际使用时使用Pillow相关库进行图片读取即可，这里我们对数据先做个模拟
        image = Image.open(image_path).convert('RGB')
        return image

    def __getitem__(self, index):
        """
        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）
        """
        image = self.load_img(self.img[index])
        label = self.label[index]
        # label = paddle.to_tensor(label)
        
        return data_transforms(image), paddle.nn.functional.label_smooth(label)

    def __len__(self):
        """
        步骤四：实现__len__方法，返回数据集总数目
        """
        return len(self.img)

#train_loader
train_dataset = MyDataset(train_img_list=train_image_path_list, val_img_list=val_image_path_list, train_label_list=train_label_list, val_label_list=val_label_list, mode='train')
train_loader = paddle.io.DataLoader(train_dataset, places=paddle.CPUPlace(), batch_size=32, shuffle=True, num_workers=0)

#val_loader
#注意括号里的参数和train_loader的区别
val_dataset = MyDataset(train_img_list=train_image_path_list, val_img_list=val_image_path_list, train_label_list=train_label_list, val_label_list=val_label_list, mode='test')
val_loader = paddle.io.DataLoader(val_dataset, places=paddle.CPUPlace(), batch_size=32, shuffle=True, num_workers=0)
```


```python
from paddle.vision import MobileNetV2

# 模型封装
network = paddle.vision.models.resnet101(num_classes=4,pretrained=True)
model = paddle.Model(network)

# 定义优化器

scheduler = paddle.optimizer.lr.LinearWarmup(
        learning_rate=0.5, warmup_steps=20, start_lr=0, end_lr=0.5, verbose=True)
#optim = paddle.optimizer.SGD(learning_rate=scheduler, parameters=model.parameters())
optim = paddle.optimizer.Adam(learning_rate=3e-4, parameters=model.parameters())

# 配置模型
model.prepare(
    optim,
    paddle.nn.CrossEntropyLoss(soft_label=True),
    Accuracy()
    )

#vdl回调函数
visualdl = paddle.callbacks.VisualDL(log_dir='visualdl_log')

# 模型训练与评估
model.fit(train_loader,
        val_loader,
        log_freq=1,
        epochs=10,
        save_dir='./chk_points/',
        callbacks=[visualdl],
        shuffle=True,
        verbose=1,
        )
```

    2021-03-10 12:16:59,589 - INFO - unique_endpoints {''}
    [INFO 2021-03-10 12:16:59,589 download.py:154] unique_endpoints {''}
    2021-03-10 12:16:59,591 - INFO - File /home/aistudio/.cache/paddle/hapi/weights/resnet101.pdparams md5 checking...
    [INFO 2021-03-10 12:16:59,591 download.py:251] File /home/aistudio/.cache/paddle/hapi/weights/resnet101.pdparams md5 checking...
    2021-03-10 12:17:00,234 - INFO - Found /home/aistudio/.cache/paddle/hapi/weights/resnet101.pdparams
    [INFO 2021-03-10 12:17:00,234 download.py:184] Found /home/aistudio/.cache/paddle/hapi/weights/resnet101.pdparams


    Epoch 0: LinearWarmup set learning rate to 0.0.
    The loss value printed in the log is the current step, and the metric is the average value of previous step.
    Epoch 1/10
    step 41/41 [==============================] - loss: 0.5962 - acc: 0.9533 - 353ms/step         
    save checkpoint at /home/aistudio/chk_points/0
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.4073 - acc: 0.9955 - 300ms/step        
    Eval samples: 221
    Epoch 2/10
    step 41/41 [==============================] - loss: 0.3910 - acc: 0.9992 - 342ms/step        
    save checkpoint at /home/aistudio/chk_points/1
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.3584 - acc: 1.0000 - 296ms/step        
    Eval samples: 221
    Epoch 3/10
    step 41/41 [==============================] - loss: 0.4890 - acc: 1.0000 - 341ms/step         
    save checkpoint at /home/aistudio/chk_points/2
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.3664 - acc: 1.0000 - 297ms/step        
    Eval samples: 221
    Epoch 4/10
    step 41/41 [==============================] - loss: 0.4458 - acc: 1.0000 - 341ms/step         
    save checkpoint at /home/aistudio/chk_points/3
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.3891 - acc: 0.9955 - 297ms/step        
    Eval samples: 221
    Epoch 5/10
    step 41/41 [==============================] - loss: 0.3666 - acc: 0.9945 - 344ms/step         
    save checkpoint at /home/aistudio/chk_points/4
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.3622 - acc: 1.0000 - 298ms/step        
    Eval samples: 221
    Epoch 6/10
    step 41/41 [==============================] - loss: 0.5063 - acc: 0.9860 - 351ms/step        
    save checkpoint at /home/aistudio/chk_points/5
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.3615 - acc: 1.0000 - 297ms/step        
    Eval samples: 221
    Epoch 7/10
    step 41/41 [==============================] - loss: 0.3690 - acc: 0.9992 - 343ms/step        
    save checkpoint at /home/aistudio/chk_points/6
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.3646 - acc: 1.0000 - 300ms/step        
    Eval samples: 221
    Epoch 8/10
    step 41/41 [==============================] - loss: 0.3787 - acc: 1.0000 - 341ms/step         
    save checkpoint at /home/aistudio/chk_points/7
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.3562 - acc: 1.0000 - 300ms/step        
    Eval samples: 221
    Epoch 9/10
    step 41/41 [==============================] - loss: 0.4566 - acc: 1.0000 - 351ms/step         
    save checkpoint at /home/aistudio/chk_points/8
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.3559 - acc: 1.0000 - 300ms/step        
    Eval samples: 221
    Epoch 10/10
    step 41/41 [==============================] - loss: 0.3803 - acc: 1.0000 - 351ms/step         
    save checkpoint at /home/aistudio/chk_points/9
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 [==============================] - loss: 0.3550 - acc: 1.0000 - 297ms/step        
    Eval samples: 221
    save checkpoint at /home/aistudio/chk_points/final



```python
# 模型保存
model.save('MyCNN', False)
```

#  {'loss': [0.35487223], 'acc': 1.0}
### 验证集 精度满分，但是从后面 test_images的测试结果还是会有误判的。


```python
# 模型评价
model.evaluate(val_loader)
```

    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 7/7 - loss: 0.3568 - acc: 1.0000 - 310ms/step
    Eval samples: 221





    {'loss': [0.3568458], 'acc': 1.0}




```python
# 模型预测
import os, time
import matplotlib.pyplot as plt
import paddle
from PIL import Image
import numpy as np

def load_image(img_path):
    '''
    预测图片预处理
    '''
    img = Image.open(img_path).convert('RGB')
    plt.imshow(img)          #根据数组绘制图像
    plt.show()               #显示图像
    
    #resize
    img = img.resize((224, 224), Image.BILINEAR) #Image.BILINEAR双线性插值
    img = np.array(img).astype('float32')

    # HWC to CHW 
    img = img.transpose((2, 0, 1))
    
    #Normalize  
    # img = img / 255         #像素值归一化
    mean = [0, 0, 0]   
    std = [255, 255, 255]
    img[0] = (img[0] - mean[0]) / std[0]
    img[1] = (img[1] - mean[1]) / std[1]
    img[2] = (img[2] - mean[2]) / std[2]
    
    return img

def infer_img(path, model_file_path, use_gpu):
    '''
    模型预测
    '''
    paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')
    model = paddle.jit.load(model_file_path)
    model.eval() #训练模式

    #对预测图片进行预处理
    infer_imgs = []
    infer_imgs.append(load_image(path))
    infer_imgs = np.array(infer_imgs)
    label_list = ['0:優良', '1:良', '2:加工品', '3:規格外']
    label_pre = []
    for i in range(len(infer_imgs)):
        data = infer_imgs[i]
        dy_x_data = np.array(data).astype('float32')
        dy_x_data = dy_x_data[np.newaxis,:, : ,:]
        img = paddle.to_tensor(dy_x_data)
        out = model(img)

        # print(out[0])
        # print(paddle.nn.functional.softmax(out)[0]) # 若模型中已经包含softmax则不用此行代码。

        lab = np.argmax(out.numpy())  #argmax():返回最大数的索引
        label_pre.append(lab)
        # print(lab)
        print("样本: {}, 预测结果:{}\n".format(path, label_list[lab]))
    return label_pre
    
infer_img(path='data/test_images/test_1604.jpg',model_file_path='MyCNN',use_gpu=1)

infer_img(path='data/train_images/train_0000.jpg',model_file_path='MyCNN',use_gpu=1)
```


![png](output_10_0.png)


    样本: data/test_images/test_1604.jpg, 预测结果:3:規格外
    



![png](output_10_2.png)


    样本: data/train_images/train_0000.jpg, 预测结果:0:優良
    





    [0]



## 接下来 输出 测试集 的结果，并生成csv文件。
## 注意：为了不超过notebook生成版本可以预览的限制（小于10MB），下面代码生成了几个图片我就中断了，展示的是部分的测试集结果，包括csv里面也是一部分


```python
# 生成csv文件
import os
import csv
zemax = []
def create_csv_2(dirname):
    path = 'data/'+ dirname +'/' 
    name = os.listdir(path)
    # 对测试集文件名排序，test_0257.jpg，split出来数字字符，int转为整数
    name.sort(key=lambda x:  (int(x.split('.')[0].split('_')[1])))
    with open (dirname+'.csv','w') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['id', 'label'])
        for n in name:
            if n[-4:] == '.jpg':
                print('测试样本：',n,)
                zemax = infer_img(path='data/test_images/'+ n,model_file_path='MyCNN',use_gpu=1)
                writer.writerow([str(n),zemax[0]])
            else:
                pass
            

create_csv_2('test_images')
```

    测试样本： test_0000.jpg



![png](output_12_1.png)


    样本: data/test_images/test_0000.jpg, 预测结果:0:優良
    
    测试样本： test_0001.jpg



![png](output_12_3.png)


    样本: data/test_images/test_0001.jpg, 预测结果:3:規格外
    
    测试样本： test_0002.jpg



![png](output_12_5.png)


    样本: data/test_images/test_0002.jpg, 预测结果:0:優良
    
    测试样本： test_0003.jpg



![png](output_12_7.png)


    样本: data/test_images/test_0003.jpg, 预测结果:3:規格外
    
    测试样本： test_0004.jpg



![png](output_12_9.png)


    样本: data/test_images/test_0004.jpg, 预测结果:0:優良
    
    测试样本： test_0005.jpg



![png](output_12_11.png)


    样本: data/test_images/test_0005.jpg, 预测结果:0:優良
    
    测试样本： test_0006.jpg



![png](output_12_13.png)


    样本: data/test_images/test_0006.jpg, 预测结果:0:優良
    
    测试样本： test_0007.jpg



![png](output_12_15.png)


    样本: data/test_images/test_0007.jpg, 预测结果:3:規格外
    
    测试样本： test_0008.jpg

